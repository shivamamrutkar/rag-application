# ğŸ“˜ Simple RAG App (Gemini + FAISS + LangChain)

A lightweight **Retrieval-Augmented Generation (RAG)** implementation using:

* **Google Gemini** for embeddings + text generation
* **FAISS** for efficient vector similarity search
* **LangChain** for pipeline management

This app lets you **upload a PDF and ask questions** about it, with Gemini grounding answers in the retrieved document chunks.

---

## ğŸ”— Link: [Live Demo](https://simple-rag.streamlit.app/)

## ğŸš€ Features

* ğŸ“‘ **PDF ingestion** â€“ extracts text from uploaded PDFs.
* âœ‚ï¸ **Chunking** â€“ splits text into chunks for retrieval.
* ğŸ” **Vector store (FAISS)** â€“ efficient semantic search over embeddings.
* ğŸ¤– **Gemini LLM** â€“ answers questions based on retrieved chunks.
* âš¡ **Simple workflow** â€“ one script, no heavy UI, just pure RAG.

---

## ğŸ“‚ Project Structure

```
simple-rag/
â”‚
â”œâ”€â”€ rag_app.py          # Main script
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ sample.pdf          # Example input PDF
â””â”€â”€ README.md           # Documentation
```

---

## ğŸ› ï¸ Installation

### 1. Clone the repo

```bash
git clone https://github.com/your-username/simple-rag.git
cd simple-rag
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt**

```
langchain
faiss-cpu
PyPDF2
google-generativeai
```

### 3. Set up API key

Export your Gemini API key as an environment variable:

```bash
export GOOGLE_API_KEY="your_api_key_here"
```

Or create a `.env` file with:

```
GOOGLE_API_KEY=your_api_key_here
```

---

## â–¶ï¸ Usage

Run the script:

```bash
python rag_app.py
```

Youâ€™ll be prompted to:

1. Upload a PDF
2. Ask a question
3. See Geminiâ€™s answer (with retrieval grounding)

---

## ğŸ§© How It Works

1. **PDF â†’ Text**: Extracts text from the PDF.
2. **Chunking**: Splits into chunks (`chunk_size`, `chunk_overlap`).
3. **Embeddings**: Converts chunks into embeddings via Gemini.
4. **Vector Store**: Stores embeddings in FAISS.
5. **Query â†’ Retrieval**: Userâ€™s question is embedded and top-`k` chunks are retrieved.
6. **LLM Generation**: Gemini answers based on question + retrieved context.

---

## ğŸ“œ License

MIT License â€” free to use, modify, and learn from.
